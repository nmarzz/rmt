''' Implements a Trainer class along with model specific trainers that are subtypes of Trainer'''

import torch
from torch import nn
from torch.utils.data import DataLoader
from argparse import Namespace
import torch.optim as optim
import numpy as np
from logger import Logger
from loss_functions import get_loss_function
import time


class Trainer():
    ''' A Trainer base class that handles universal aspects of training.

    Such as:
        - The training loop
        - Logging
        - training instantiation

    Each subclass of Trainer must include a
        - train_epoch function
        - validate function

    '''

    def __init__(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, device: torch.device, logger: Logger, idx: int, args: Namespace):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.early_stop = args.early_stop
        self.lr = args.lr
        self.epochs = args.epochs

        self.iters = []
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.train_accs5 = []
        self.val_accs5 = []
        self.idx = idx
        self.logger = logger
        self.eval_set = 'Validation Set'
        self.model_path = logger.get_model_path()
        self.log_dir = logger.get_log_dir()
        self.num_devices = len(args.device)  
        self.clip = 1000              

        self.model.to(self.device)

        self.loss_name = args.loss_function
        self.loss_function = get_loss_function(args.loss_function)
        if args.optimizer == 'sgd':
            self.optimizer = optim.SGD(
                model.parameters(), lr=args.lr, momentum=args.momentum)
        elif args.optimizer == 'adam':
            self.optimizer = optim.Adam(
                model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
        else:
            raise ValueError('Optimizer not supported')
        
    def train(self):
        epochs_until_stop = self.early_stop

        # Get losses at initialization
        val_loss, val_acc, val_acc5 = self.validate()
        train_loss, train_acc, train_acc5 = self.validate(train=True)
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        self.train_accs.append(train_acc)
        self.val_accs.append(val_acc)
        self.train_accs5.append(train_acc5)
        self.val_accs5.append(val_acc5)

        train_start = time.time()
        for epoch in range(1, self.epochs + 1):
            epoch_start = time.time()
            train_loss, train_acc, train_acc5 = self.train_epoch(epoch)
            self.logger.log('Epoch time: {} s'.format(time.time() - epoch_start))
            val_loss, val_acc, val_acc5 = self.validate()

            self.logger.log('\n')
            self.logger.log(f'Training: Average Loss {train_loss}')
            if train_acc is not None:
                self.logger.log(
                    'Training set: Average top-1 accuracy: {:.2f}'.format(train_acc))
                self.logger.log(
                    'Training set: Average top-5 accuracy: {:.2f}'.format(train_acc5))
                self.logger.log(
                    'Validation set: Average loss: {:.6f}'.format(val_loss))
                self.logger.log('\n')

            if val_acc is not None:
                self.logger.log(
                    'Validation set: Top-1 Accuracy: {:.2f}'.format(val_acc))
                self.logger.log(
                    'Validation set: Top-5 Accuracy: {:.2f}'.format(val_acc5))
                self.logger.log('\n')
                self.logger.log('\n')

            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accs.append(train_acc)
            self.val_accs.append(val_acc)
            self.train_accs5.append(train_acc5)
            self.val_accs5.append(val_acc5)

            # Check if validation accuracy is worsening
            if val_acc <= max(self.val_accs):
                epochs_until_stop -= 1
                if epochs_until_stop == 0:  # Early stopping initiated
                    break
            else:
                epochs_until_stop = self.early_stop
                if self.idx == 0:
                    # Save model when loss improves
                    current_model_path = self.model_path
                    self.logger.log("Saving model...")

                    if self.num_devices > 1:
                        torch.save(self.model.module.state_dict(),
                                   current_model_path)
                    else:
                        torch.save(self.model.state_dict(), current_model_path)
                    self.logger.log("Model saved.\n")

        self.logger.log('Training time: {} s'.format(time.time() - train_start))

        if self.idx == 0:
            self.train_report()
            self.logger.log_results(self.train_losses, self.val_losses,
                                    self.train_accs, self.val_accs, self.train_accs5, self.val_accs5)

    def train_report(self):
        best_epoch = np.argmax(self.val_accs)
        self.logger.log("Training complete.\n")
        self.logger.log("Best Epoch: {}".format(best_epoch))
        self.logger.log("Training Loss: {:.6f}".format(
            self.train_losses[best_epoch]))
        if self.train_accs[best_epoch] is not None:
            self.logger.log("Training Top-1 Accuracy: {:.2f}".format(
                self.train_accs[best_epoch]))
            self.logger.log("Training Top-5 Accuracy: {:.2f}".format(
                self.train_accs5[best_epoch]))
        self.logger.log("{} Loss: {:.6f}".format(
            self.eval_set, self.val_losses[best_epoch]))
        if self.val_accs[best_epoch] is not None:
            self.logger.log("{} Top-1 Accuracy: {:.2f}".format(
                self.eval_set, self.val_accs[best_epoch]))
            self.logger.log("{} Top-5 Accuracy: {:.2f}".format(
                self.eval_set, self.val_accs5[best_epoch]))

    def train_epoch(self, epoch):
        pass

    def validate(self):
        pass


class GeneralTrainer(Trainer):
    def __init__(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, device: torch.device, logger: Logger, idx: int, args: Namespace):
        super().__init__(model, train_loader, val_loader, device, logger, idx, args)

    def train_epoch(self, epoch):
        self.model.train()
        train_loss = AverageMeter()
        train_top1_acc = AverageMeter()
        train_top5_acc = AverageMeter()
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device, non_blocking = True), target.to(self.device, non_blocking = True)
            self.optimizer.zero_grad(set_to_none= True)

            output = self.model(data)
            loss = self.loss_function(output, target)
            top1_acc, top5_acc = compute_accuracy(output, target)
            train_loss.update(loss.item())
            train_top1_acc.update(top1_acc)
            train_top5_acc.update(top5_acc)
            
            loss.backward()          
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)                  
            
            self.optimizer.step()            
            
            if batch_idx % 10 == 0:
                logged_loss = train_loss.get_avg()

                log_str = 'Train Epoch {} [{}/{} ({:.0f}%)]\tLoss: {:6f}'.format(
                    epoch, batch_idx *
                    len(data), int(
                        len(self.train_loader.dataset) / self.num_devices),
                    100. * batch_idx / len(self.train_loader), logged_loss)
                self.logger.log(log_str)

        return train_loss.get_avg(), train_top1_acc.get_avg(), train_top5_acc.get_avg()

    def validate(self, train: bool = False):
        return predict(self.model, self.device, self.val_loader, self.loss_function)


class AverageMeter():
    """Computes and stores the average and current value

    Taken from the Torch examples repository:
    https://github.com/pytorch/examples/blob/master/imagenet/main.py"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

    def get_avg(self):
        return self.avg


def compute_accuracy(output, target):
    ''' Helper function that computes accuracy '''
    with torch.no_grad():
        batch_size = target.shape[0]

        _, pred = output.topk(5, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))
        top1_acc = correct[:1].view(-1).float().sum(0,
                                                    keepdim=True) * 100.0 / batch_size
        top5_acc = correct[:5].reshape(-1).float().sum(0,
                                                       keepdim=True) * 100.0 / batch_size

    return top1_acc.item(), top5_acc.item()


def predict(model: nn.Module, device: torch.device,
            loader: torch.utils.data.DataLoader, loss_function: nn.Module,
            precision: str = '32', calculate_confusion: bool = False) -> tuple([float, float]):
    """Evaluate supervised model on data.

    Args:
        model: Model to be evaluated.
        device: Device to evaluate on.
        loader: Data to evaluate on.
        loss_function: Loss function being used.
        precision: precision to evaluate model with

    Returns:
        Model loss and accuracy on the evaluation dataset.

    """
    model.eval()

    total_loss = 0
    acc1 = 0
    acc5 = 0
    confusion = []
    with torch.no_grad():
        for data, target in loader:
            data, target = data.to(device, non_blocking = True), target.to(device, non_blocking = True)
            
            output = model(data)            
            loss = loss_function(output, target)
            total_loss += loss.item()

            cur_acc1, cur_acc5 = compute_accuracy(output, target)
            acc1 += cur_acc1
            acc5 += cur_acc5

    total_loss, acc1, acc5 = total_loss / len(loader), acc1 / \
        len(loader), acc5 / len(loader)

    if calculate_confusion:
        confusion = np.mean(np.stack(confusion), axis=0)
        return total_loss, acc1, acc5, confusion.round(2)
    else:
        return total_loss, acc1, acc5
